{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d3a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# British Airways Reviews Pipeline (Collection + Cleaning)\n",
    "# ===============================\n",
    "\n",
    "# ---- Imports ----\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ---- Step 1: Data Collection ----\n",
    "all_reviews, all_ratings, all_dates, all_countries = [], [], [], []\n",
    "\n",
    "# Collect reviews from Skytrax (35 pages, 100 per page)\n",
    "for page_no in range(1, 36):\n",
    "    response = requests.get(\n",
    "        f\"https://www.airlinequality.com/airline-reviews/british-airways/page/{page_no}/?sortby=post_date%3ADesc&pagesize=100\"\n",
    "    )\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract review texts\n",
    "    for block in soup.find_all(\"div\", class_=\"text_content\"):\n",
    "        all_reviews.append(block.text)\n",
    "\n",
    "    # Extract ratings\n",
    "    for block in soup.find_all(\"div\", class_=\"rating-10\"):\n",
    "        try:\n",
    "            all_ratings.append(block.span.text)\n",
    "        except:\n",
    "            print(f\"Rating missing on page {page_no}\")\n",
    "            all_ratings.append(\"None\")\n",
    "\n",
    "    # Extract review date\n",
    "    for block in soup.find_all(\"time\"):\n",
    "        all_dates.append(block.text)\n",
    "\n",
    "    # Extract reviewer country\n",
    "    for block in soup.find_all(\"h3\"):\n",
    "        all_countries.append(block.span.next_sibling.text.strip(\" ()\"))\n",
    "\n",
    "# Trim ratings to match length (small adjustment if mismatch)\n",
    "all_ratings = all_ratings[:len(all_reviews)]\n",
    "\n",
    "# Create raw dataframe\n",
    "raw_df = pd.DataFrame({\n",
    "    \"review_text\": all_reviews,\n",
    "    \"rating\": all_ratings,\n",
    "    \"review_date\": all_dates,\n",
    "    \"reviewer_country\": all_countries\n",
    "})\n",
    "\n",
    "print(\"ðŸ“Œ Raw Data Shape:\", raw_df.shape)\n",
    "display(raw_df.head())\n",
    "\n",
    "\n",
    "# ---- Step 2: Data Cleaning ----\n",
    "clean_df = raw_df.copy()\n",
    "\n",
    "# Mark verified reviews\n",
    "clean_df[\"is_verified\"] = clean_df.review_text.str.contains(\"Trip Verified\")\n",
    "\n",
    "# Clean textual reviews\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "cleaned_texts = []\n",
    "for txt in clean_df.review_text:\n",
    "    txt = txt.replace(\"âœ… Trip Verified |\", \"\")\n",
    "    txt = re.sub(\"[^a-zA-Z]\", \" \", txt).lower()\n",
    "    tokens = txt.split()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    cleaned_texts.append(\" \".join(tokens))\n",
    "\n",
    "clean_df[\"cleaned_review\"] = cleaned_texts\n",
    "\n",
    "# Convert date to datetime\n",
    "clean_df[\"review_date\"] = pd.to_datetime(clean_df[\"review_date\"], errors=\"coerce\")\n",
    "\n",
    "# Clean ratings (remove junk, drop \"None\")\n",
    "clean_df[\"rating\"] = clean_df[\"rating\"].str.strip()\n",
    "clean_df = clean_df[clean_df[\"rating\"] != \"None\"]\n",
    "\n",
    "# Handle missing countries\n",
    "clean_df = clean_df.dropna(subset=[\"reviewer_country\"])\n",
    "\n",
    "# Reset index\n",
    "clean_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"ðŸ“Œ Cleaned Data Shape:\", clean_df.shape)\n",
    "display(clean_df.head())\n",
    "\n",
    "\n",
    "# ---- Step 3: Save Clean Data ----\n",
    "output_path = os.path.join(os.getcwd(), \"cleaned_ba_reviews.csv\")\n",
    "clean_df.to_csv(output_path, index=False)\n",
    "print(\"âœ… Cleaned dataset exported to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfae4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Exploratory Data Analysis (EDA)\n",
    "# ===============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction import text, TfidfVectorizer\n",
    "import nltk\n",
    "\n",
    "# Load the cleaned dataset\n",
    "eda_df = pd.read_csv(\"cleaned_ba_reviews.csv\")\n",
    "eda_df = eda_df.reset_index(drop=True)\n",
    "\n",
    "print(\"ðŸ“Š Dataset Shape:\", eda_df.shape)\n",
    "display(eda_df.head())\n",
    "\n",
    "\n",
    "# ---- Ratings Analysis ----\n",
    "print(\"â­ Average Rating:\", eda_df[\"rating\"].astype(int).mean())\n",
    "\n",
    "rating_counts = eda_df[\"rating\"].value_counts().sort_index()\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=rating_counts.index, y=rating_counts.values, palette=\"viridis\")\n",
    "plt.xlabel(\"Rating (Stars)\")\n",
    "plt.ylabel(\"Number of Reviews\")\n",
    "plt.title(\"Distribution of Ratings\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---- Country Insights ----\n",
    "print(f\"ðŸŒ Reviews from {eda_df['reviewer_country'].nunique()} unique countries\")\n",
    "\n",
    "country_review_counts = eda_df[\"reviewer_country\"].value_counts().head(10)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=country_review_counts.index, y=country_review_counts.values, palette=\"magma\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.title(\"Top 10 Countries by Review Count\")\n",
    "plt.show()\n",
    "\n",
    "# Average ratings per country (top 12)\n",
    "country_avg_ratings = (\n",
    "    eda_df.groupby(\"reviewer_country\")[\"rating\"].mean().sort_values(ascending=False).head(12)\n",
    ")\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.barplot(x=country_avg_ratings.index, y=country_avg_ratings.values, palette=\"coolwarm\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.title(\"Top 12 Countries with Highest Avg Ratings\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---- Time Series of Ratings ----\n",
    "eda_df[\"review_date\"] = pd.to_datetime(eda_df[\"review_date\"], errors=\"coerce\")\n",
    "fig = px.line(eda_df, x=\"review_date\", y=eda_df[\"rating\"].astype(int), title=\"Ratings Over Time\")\n",
    "fig.update_xaxes(rangeslider_visible=True)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# ---- WordCloud ----\n",
    "all_text = \" \".join(eda_df[\"cleaned_review\"])\n",
    "custom_stopwords = set(stopwords.words(\"english\"))\n",
    "custom_stopwords.update([\n",
    "    \"ba\", \"flight\", \"british\", \"airway\", \"airline\", \"plane\", \"london\", \"heathrow\",\n",
    "    \"passenger\", \"aircraft\", \"would\", \"could\"\n",
    "])\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "wc = WordCloud(width=1000, height=600, stopwords=custom_stopwords, max_words=300, background_color=\"white\").generate(all_text)\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"WordCloud of Customer Reviews\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---- Word Frequency ----\n",
    "words_list = [w for w in all_text.split() if w not in text.ENGLISH_STOP_WORDS]\n",
    "freq_dist = FreqDist(words_list).most_common(20)\n",
    "\n",
    "freq_df = pd.DataFrame(freq_dist, columns=[\"Word\", \"Frequency\"])\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x=\"Word\", y=\"Frequency\", data=freq_df, palette=\"crest\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.title(\"Top 20 Frequent Words in Reviews\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---- N-grams Function ----\n",
    "from nltk import ngrams\n",
    "\n",
    "def plot_ngrams(words, n=2, top_k=15):\n",
    "    ngram_list = ngrams(words, n)\n",
    "    freq = FreqDist(ngram_list).most_common(top_k)\n",
    "    freq_dict = {\"_\".join(k): v for k, v in freq}\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(x=list(freq_dict.keys()), y=list(freq_dict.values()), palette=\"flare\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(f\"Top {top_k} {n}-grams\")\n",
    "    plt.show()\n",
    "\n",
    "# Example: Show top bigrams\n",
    "plot_ngrams(words_list, n=2)\n",
    "\n",
    "\n",
    "# ---- Sentiment Analysis ----\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Polarity with TextBlob\n",
    "eda_df[\"polarity\"] = eda_df[\"cleaned_review\"].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Vader sentiment labels\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "eda_df[\"sentiment_label\"] = eda_df[\"cleaned_review\"].apply(\n",
    "    lambda x: 1 if sid.polarity_scores(x)[\"compound\"] > 0.2 else (-1 if sid.polarity_scores(x)[\"compound\"] < -0.2 else 0)\n",
    ")\n",
    "\n",
    "print(\"ðŸ” Sentiment Label Distribution:\")\n",
    "print(eda_df[\"sentiment_label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938a4f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Customer Booking Data Analysis (EDA & Cleaning)\n",
    "# ===============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Load dataset\n",
    "cwd = os.getcwd()\n",
    "booking_df = pd.read_csv(cwd + \"/customer_booking.csv\", encoding=\"ISO-8859-1\")\n",
    "booking_df = booking_df.reset_index(drop=True)\n",
    "\n",
    "print(\"ðŸ“Œ Booking Dataset Shape:\", booking_df.shape)\n",
    "display(booking_df.head())\n",
    "booking_df.info()\n",
    "booking_df.describe()\n",
    "\n",
    "\n",
    "# ---- Sales Channel Analysis ----\n",
    "sales_counts = booking_df[\"sales_channel\"].value_counts()\n",
    "total_sales = sales_counts.sum()\n",
    "for channel, count in sales_counts.items():\n",
    "    print(f\"Bookings via {channel}: {count} ({round(count/total_sales*100,2)}%)\")\n",
    "\n",
    "\n",
    "# ---- Trip Type Distribution ----\n",
    "trip_counts = booking_df[\"trip_type\"].value_counts()\n",
    "total_trips = trip_counts.sum()\n",
    "for trip, count in trip_counts.items():\n",
    "    print(f\"Trip type '{trip}': {round(count/total_trips*100,2)}% of bookings\")\n",
    "\n",
    "\n",
    "# ---- Purchase Lead Analysis ----\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.histplot(booking_df[\"purchase_lead\"], bins=40, kde=True, color=\"skyblue\")\n",
    "plt.title(\"Distribution of Purchase Lead (Days in Advance)\")\n",
    "plt.xlabel(\"Purchase Lead (Days)\")\n",
    "plt.ylabel(\"Number of Bookings\")\n",
    "plt.show()\n",
    "\n",
    "# Remove extreme outliers (>600 days)\n",
    "booking_df = booking_df[booking_df[\"purchase_lead\"] <= 600]\n",
    "\n",
    "\n",
    "# ---- Length of Stay Analysis ----\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.histplot(booking_df[\"length_of_stay\"], bins=40, kde=True, color=\"salmon\")\n",
    "plt.title(\"Distribution of Length of Stay\")\n",
    "plt.xlabel(\"Length of Stay (Days)\")\n",
    "plt.ylabel(\"Number of Bookings\")\n",
    "plt.show()\n",
    "\n",
    "# Remove extreme outliers (>500 days)\n",
    "booking_df = booking_df[booking_df[\"length_of_stay\"] <= 500]\n",
    "\n",
    "\n",
    "# ---- Flight Day Mapping ----\n",
    "day_map = {\"Mon\":1, \"Tue\":2, \"Wed\":3, \"Thu\":4, \"Fri\":5, \"Sat\":6, \"Sun\":7}\n",
    "booking_df[\"flight_day_num\"] = booking_df[\"flight_day\"].map(day_map)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.countplot(x=\"flight_day_num\", data=booking_df, palette=\"viridis\")\n",
    "plt.title(\"Booking Count by Flight Day\")\n",
    "plt.xlabel(\"Day of Week (1=Mon, 7=Sun)\")\n",
    "plt.ylabel(\"Number of Bookings\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---- Booking Origin ----\n",
    "top_origins = booking_df[\"booking_origin\"].value_counts().head(20)\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.barplot(x=top_origins.index, y=top_origins.values, palette=\"magma\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Top 20 Countries by Booking Requests\")\n",
    "plt.xlabel(\"Country\")\n",
    "plt.ylabel(\"Number of Bookings\")\n",
    "plt.show()\n",
    "\n",
    "# Successful bookings by country\n",
    "top_complete_origins = booking_df[booking_df[\"booking_complete\"]==1][\"booking_origin\"].value_counts().head(20)\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.barplot(x=top_complete_origins.index, y=top_complete_origins.values, palette=\"coolwarm\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Top 20 Countries by Completed Bookings\")\n",
    "plt.xlabel(\"Country\")\n",
    "plt.ylabel(\"Number of Completed Bookings\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---- Booking Completion Rate ----\n",
    "completion_rate = booking_df[\"booking_complete\"].mean() * 100\n",
    "print(f\"âœ… Booking Completion Rate: {round(completion_rate,2)}%\")\n",
    "print(f\"âŒ Incomplete Bookings: {round(100-completion_rate,2)}%\")\n",
    "\n",
    "\n",
    "# ---- Save Cleaned/Filtered Dataset ----\n",
    "output_file = os.path.join(cwd, \"filtered_customer_booking.csv\")\n",
    "booking_df.to_csv(output_file, index=False)\n",
    "print(\"âœ… Filtered dataset saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37e3472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Predictive Modeling: Customer Booking Completion\n",
    "# ===============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "\n",
    "# Load cleaned customer booking dataset\n",
    "cwd = os.getcwd()\n",
    "booking_df = pd.read_csv(cwd + \"/filtered_customer_booking.csv\")\n",
    "booking_df = booking_df.reset_index(drop=True)\n",
    "display(booking_df.head())\n",
    "\n",
    "# -------------------------------\n",
    "# One-Hot Encoding for categorical variables\n",
    "# -------------------------------\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "# Sales Channel\n",
    "sales_encoded = pd.DataFrame(\n",
    "    encoder.fit_transform(booking_df[['sales_channel']]),\n",
    "    columns=['Internet','Mobile']\n",
    ")\n",
    "# Trip Type\n",
    "trip_encoded = pd.DataFrame(\n",
    "    encoder.fit_transform(booking_df[['trip_type']]),\n",
    "    columns=['RoundTrip','OneWayTrip','CircleTrip']\n",
    ")\n",
    "\n",
    "# Combine encoded columns\n",
    "booking_processed = booking_df.join([sales_encoded, trip_encoded])\n",
    "booking_processed.drop(['sales_channel','trip_type','booking_origin','route'], axis=1, inplace=True)\n",
    "\n",
    "# Target label\n",
    "y = booking_processed.pop('booking_complete')\n",
    "X = booking_processed.copy()\n",
    "\n",
    "# -------------------------------\n",
    "# Feature Scaling\n",
    "# -------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "X_scaled['label'] = y\n",
    "\n",
    "# -------------------------------\n",
    "# Correlation Matrix (optional visual check)\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(X_scaled.corr(), cmap='coolwarm', annot=False)\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Train-Test Split\n",
    "# -------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled.drop('label', axis=1).to_numpy(),\n",
    "    X_scaled['label'].to_numpy(),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Helper functions for modeling\n",
    "# -------------------------------\n",
    "def fit_predict(model, X_tr, y_tr, X_val):\n",
    "    model.fit(X_tr, y_tr)\n",
    "    return model.predict(X_val)\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    return {\n",
    "        \"accuracy\": round(accuracy_score(y_true, y_pred), 2),\n",
    "        \"precision\": round(precision_score(y_true, y_pred), 2),\n",
    "        \"recall\": round(recall_score(y_true, y_pred), 2),\n",
    "        \"f1\": round(f1_score(y_true, y_pred), 2)\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# Random Forest Classifier\n",
    "# -------------------------------\n",
    "rf_clf = RandomForestClassifier(n_estimators=50, max_depth=50, min_samples_split=5, random_state=0)\n",
    "\n",
    "# Training Evaluation\n",
    "y_train_pred = fit_predict(rf_clf, X_train, y_train, X_train)\n",
    "train_metrics = evaluate_model(y_train, y_train_pred)\n",
    "print(\"âœ… Training Metrics:\", train_metrics)\n",
    "\n",
    "# Confusion Matrix - Training\n",
    "cm_train = ConfusionMatrix(rf_clf, classes=[0,1])\n",
    "cm_train.fit(X_train, y_train)\n",
    "cm_train.score(X_train, y_train)\n",
    "\n",
    "# Testing Evaluation\n",
    "y_test_pred = fit_predict(rf_clf, X_train, y_train, X_test)\n",
    "test_metrics = evaluate_model(y_test, y_test_pred)\n",
    "print(\"ðŸ“Š Testing Metrics:\", test_metrics)\n",
    "\n",
    "# Confusion Matrix - Testing\n",
    "cm_test = ConfusionMatrix(rf_clf, classes=[0,1])\n",
    "cm_test.fit(X_train, y_train)\n",
    "cm_test.score(X_test, y_test)\n",
    "\n",
    "# Feature Importance Plot\n",
    "plt.figure(figsize=(10,8))\n",
    "feature_importances = rf_clf.feature_importances_\n",
    "sorted_idx = feature_importances.argsort()\n",
    "plt.barh(X_scaled.drop('label', axis=1).columns[sorted_idx], feature_importances[sorted_idx])\n",
    "plt.xlabel(\"Random Forest Feature Importance\")\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Handling Imbalanced Dataset\n",
    "# -------------------------------\n",
    "print(\"Original label distribution:\\n\", X_scaled['label'].value_counts())\n",
    "\n",
    "# Downsample majority class (label=0) to balance\n",
    "majority_df = X_scaled[X_scaled['label']==0].sample(n=8000, random_state=42)\n",
    "minority_df = X_scaled[X_scaled['label']==1]\n",
    "balanced_df = pd.concat([majority_df, minority_df], ignore_index=True).sample(frac=1, random_state=42)\n",
    "\n",
    "X_bal = balanced_df.drop('label', axis=1)\n",
    "y_bal = balanced_df['label']\n",
    "\n",
    "# Split balanced data\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(\n",
    "    X_bal.to_numpy(), y_bal.to_numpy(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train on balanced data\n",
    "rf_bal = RandomForestClassifier(n_estimators=50, max_depth=50, min_samples_split=5, random_state=0)\n",
    "y_test_pred_bal = fit_predict(rf_bal, X_train_bal, y_train_bal, X_test_bal)\n",
    "\n",
    "# Evaluate balanced model\n",
    "balanced_metrics = evaluate_model(y_test_bal, y_test_pred_bal)\n",
    "print(\"âœ… Balanced Dataset Testing Metrics:\", balanced_metrics)\n",
    "\n",
    "# Confusion Matrix - Balanced Data\n",
    "cm_bal = ConfusionMatrix(rf_bal, classes=[0,1])\n",
    "cm_bal.fit(X_train_bal, y_train_bal)\n",
    "cm_bal.score(X_test_bal, y_test_bal)\n",
    "\n",
    "# Feature Importance - Balanced Data\n",
    "plt.figure(figsize=(10,8))\n",
    "feat_imp_bal = rf_bal.feature_importances_\n",
    "sorted_idx_bal = feat_imp_bal.argsort()\n",
    "plt.barh(X_bal.columns[sorted_idx_bal], feat_imp_bal[sorted_idx_bal])\n",
    "plt.xlabel(\"Random Forest Feature Importance\")\n",
    "plt.title(\"Balanced Data Feature Importance\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
